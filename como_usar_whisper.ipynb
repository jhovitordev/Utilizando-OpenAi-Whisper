{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizando OpenAi Whisper\n",
        "\n",
        "Neste notebook, forneceremos o link de um vídeo do YouTube e demonstraremos como convertê-lo facilmente em áudio para transcrevê-lo para texto utilizando o Whisper da OpenAi.\n",
        "\n"
      ],
      "metadata": {
        "id": "2A1KnwWtOSRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando pacotes e importando bibliotecas:\n",
        "\n",
        "- yt-dlp é um downloader de áudio/vídeo de linha de comando rico de recursos com suporte para milhares de sites .\n",
        "- Whisper é um sistema de inteligência artificial criado para transformar fala em texto automaticamente."
      ],
      "metadata": {
        "id": "oKLZNrLWuKSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mONt3UlSONT9",
        "outputId": "51c4e1c6-3369-4d8e-cf5b-0ee3795e60b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/3.2 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install yt_dlp\n",
        "!pip -q install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper"
      ],
      "metadata": {
        "id": "TvtMrHC_uR7a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baixando audio e convertendo para texto\n"
      ],
      "metadata": {
        "id": "JjHxLx0CXT6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Passando o link do video do Youtube\n",
        "link = 'https://www.youtube.com/watch?v=s-Ue8SdQy3I'"
      ],
      "metadata": {
        "id": "WOEqn-4saaDR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para baixar um vídeo e convertê-lo para áudio, utilizamos a ferramenta `yt-dlp`. Para isso, configuramos a linha de comando com as opções desejadas. A sintaxe básica do comando é:\n",
        "\n",
        "`yt-dlp [OPTIONS] [--] URL [URL...]`\n",
        "\n",
        "Usaremos então as seguintes opções:\n",
        "\n",
        "- `!yt-dlp` → O ! indica que esse comando está sendo executado dentro de um ambiente interativo, como um notebook Jupyter.\n",
        "\n",
        "- `-x (ou --extract-audio)` → Extrai apenas o áudio do vídeo.\n",
        "\n",
        "- `-f bestaudio` → Especifica o formato de download como bestaudio, ou seja, a melhor qualidade de áudio disponível no vídeo.\n",
        "\n",
        "- `--audio-format mp3` → Converte o áudio extraído para MP3 após o download.\n",
        "\n",
        "- `-o` → Define o nome do arquivo de saída:\n",
        "\n",
        "- `\"audio.%(ext)s\"` → O nome do arquivo será \"audio.mp3\", pois %(ext)s será substituído pela extensão do formato de áudio escolhido (mp3).\n",
        "\n",
        "- `$link` → Representa a variável contendo a URL do vídeo, que será fornecida no momento da execução."
      ],
      "metadata": {
        "id": "CmEqJmmjbtYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Utilizando yt-dlp para baixar o audio\n",
        "!yt-dlp -x -f bestaudio --audio-format mp3 -o \"audio.%(ext)s\" $link"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYr6gSfAOkNi",
        "outputId": "0e8c80e4-b260-453c-b06f-0f90e22f2d8e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=s-Ue8SdQy3I\n",
            "[youtube] s-Ue8SdQy3I: Downloading webpage\n",
            "[youtube] s-Ue8SdQy3I: Downloading tv client config\n",
            "[youtube] s-Ue8SdQy3I: Downloading player b191cf34\n",
            "[youtube] s-Ue8SdQy3I: Downloading tv player API JSON\n",
            "[youtube] s-Ue8SdQy3I: Downloading ios player API JSON\n",
            "[youtube] s-Ue8SdQy3I: Downloading m3u8 information\n",
            "[info] s-Ue8SdQy3I: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "\u001b[K[download] 100% of   10.89MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m15.63MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para transcrever um áudio com Whisper, é importante entender que ele possui seis tipos de modelos, cada um equilibrando velocidade e precisão. O desempenho e a qualidade da transcrição podem variar conforme o modelo escolhido. Para mais detalhes, consulte a documentação.  \n",
        "\n",
        "- `whisper.load_model()`: Carrega o modelo escolhido para transcrição.  \n",
        "- `transcribe()`: Lê todo o arquivo de áudio e o divide em trechos de 30 segundos, analisando cada parte separadamente para prever o que foi dito com base no contexto."
      ],
      "metadata": {
        "id": "686dUbjY6KIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#carregando modelo\n",
        "modelo = whisper.load_model(\"large\")\n",
        "#Realizando a transcrição\n",
        "transcricao = modelo.transcribe('./audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mimtPGldYsW",
        "outputId": "fd26205a-7d27-4d8a-fe91-2ae67fbffdec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [01:07<00:00, 45.7MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcricao['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EUjUnD3KgIIT",
        "outputId": "638aee5e-3c70-4ddc-9d70-77c1bc7a472a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Fala impressionadores, tudo bem? No vídeo de hoje a gente vai falar sobre dois termos que estão cada vez mais importantes dentro da ciência de dados que são redes neurais e Deep Learning e eu vou explicar para vocês todos os principais conceitos teóricos seja para você explicar em uma entrevista, seja para você falar com o recrutador ou até implementar isso dentro da sua própria empresa e a gente também percebe que cada vez mais vagas estão pedindo por conhecimento de Deep Learning e por isso é tão importante vocês saberem desses conceitos mas antes da gente seguir eu vou pedir para vocês deixarem uma curtida nesse vídeo e baixar esse material que está aqui na descrição então, bora lá! Quando a gente fala do conceito de redes neurais a gente quer basicamente fazer com que as máquinas consigam reproduzir o comportamento do nosso cérebro o nosso cérebro atualmente é uma grande máquina de aprender com uma grande capacidade de aprender novas coisas desde pequeno a gente aprende como andar, como falar, como recrutar como reconhecer objetos, nós somos ótimos reconhecedores de objetos também como fazer várias outras coisas e a grande pergunta feita foi por que não se basear em uma estrutura que já sabe fazer isso e trazer essa mesma estrutura para dentro de uma máquina para que as máquinas aprendam assim como os humanos aprendem tá Lucas, mas como os humanos aprendem? vamos supor que você é criancinha, você é um bebê e você vê uma vela essa vela tem uma chama muito brilhante e a sua primeira ideia vai ser tentar colocar a mão naquela chama naquela coisa que brilha e quando você fizer isso na primeira vez você vai se queimar vai acontecer isso só que a partir desse momento, a partir do momento que você sente esse calor e você se queima você recebe um estímulo no seu cérebro que a partir das próximas vezes você não vai chegar tão perto e nesse momento que você evita chegar perto porque você sabe que uma coisa ruim pode acontecer a gente diz que você aprendeu nós estamos o tempo todo desenvolvendo nosso conhecimento através da experiência que você tem e você aprendeu nós estamos o tempo todo desenvolvendo nosso conhecimento através da experiência que você tem e você aprendeu e tudo que acontece com a gente é um aprendizado e agora você deve estar se perguntando tá, mas como que a gente consegue aprender tanta coisa? o que que faz o cérebro tão especial? exatamente essa pequena estrutura aqui que se chama neurônios os neurônios são estruturas relativamente simples que só vão mandar informações de sim ou não para outros neurônios ou seja, eles escolhem apenas ou passar ou não passar uma informação não vou entrar tanto na parte biológica não vou entrar tanto na parte biológica mas vocês precisam entender só isso que os neurônios tem duas principais funções primeiro, armazenar informações segundo, transmitir informações mas se é uma estrutura tão simples como que ele consegue aprender tanta coisa? exatamente devido a grande quantidade de neurônios e a sua capacidade de paralelismo ou seja, a gente tem vários neurônios paralelos, interligados fazendo toda essa comunicação e desenvolvendo todo esse conhecimento que a gente tem e o que a gente pensou foi tá, já que o cérebro faz isso de uma forma excelente com os neurônios porque que a gente não faz isso na máquina? e o que a gente fez foi realmente tentar replicar essa mesma estrutura pegamos nós que funcionam como neurônios e a cada nó a gente foi criando novas e novas conexões até ter um sistema totalmente interligado que a gente chama de redes neurais e o grande objetivo das redes neurais é tentar simular o funcionamento do cérebro deixando que esses neurônios artificiais se organizem sozinhos única coisa que a gente vai passar para eles é os nossos dados de entrada que a gente chama de input os nossos dados de saída e aqui todas essas camadas ocultas elas vão aprender por si mesmo elas vão aprender com a experiência e elas vão se desenvolvendo até conseguirem aprender com toda a informação que foi passada e essas camadas ocultas a gente pode ter várias camadas quantas camadas a gente quiser mas dependendo do poder de processamento que a gente vai falar daqui a hora e quando a gente tem uma rede neural com várias camadas a gente chama de Deep Learning e aí só para a gente entender todas essas nomenclaturas que às vezes a gente fala e não fica muito claro o que é cada coisa o Deep Learning é uma sub-área dentro do Machine Learning e o Machine Learning é uma sub-área dentro da Inteligência Artificial tá Lucas, mas o que é cada um deles? a Inteligência Artificial é basicamente quando a gente quer fazer com que os computadores possam realizar tarefas como sempre como seres humanos a gente quer que o computador consiga falar consiga executar ações consiga responder as pessoas consiga aprender tudo isso é Inteligência Artificial dentro da Inteligência Artificial a gente tem uma sub-área que é o Machine Learning que é quando o aprendizado do computador é feito através de dados nós utilizamos dados para resolver problemas e os computadores vão aprendendo e vão evoluindo com os dados que eles recebem já no Deep Learning a gente tem uma abstração um pouco maior porque enquanto no Machine Learning em geral a gente foca muito em problemas que a gente consegue ali modelar a gente consegue ter uma estrutura lógica na nossa cabeça no Deep Learning a gente vai começar a tentar resolver problemas que por mais que a gente saiba muito bem como fazer como seres humanos a gente não consegue colocar de forma tão clara em regras ou seja, a gente nem entende muito bem como a gente faz por exemplo quando a gente quer falar a gente não precisa pensar tantas coisas na hora de falar na hora de traduzir um idioma ou até mesmo na hora de reconhecer uma imagem quando a gente precisa reconhecer uma imagem a gente simplesmente vai lá e reconhece a gente fala isso é um gato isso é um cachorro isso é um porco e a gente não pensa muitas vezes em todo o processamento que o nosso cérebro está fazendo então para conseguir resolver esses problemas bem mais complexos e bem mais abstratos é que a gente usa o Deep Learning e deixa que esses neurônios do nosso modelo aprendam sozinho e sozinho eles sejam capazes de realizar todas essas tarefas e aí só falando um pouquinho de datas a inteligência artificial começou lá na década de 50 então já tem pesquisas de muito tempo o Machine Learning foi mais ali para a década de 80 década de 90 e o Deep Learning é bem mais recente não quer dizer que o Deep Learning só começou agora ele tem ali as primeiras ideias na década de 50 na década de 60 só que não existiu o poder computacional atual para fazer esses modelos realmente serem eficientes hoje com o poder computacional e com uma grande quantidade de dados a gente consegue utilizar Deep Learning com grande escala e com grande facilidade e aí só para a gente fechar essa parte é até uma provocação que eu quero fazer em vocês de vocês pensarem e se eu te falar você consegue separar o que é banana e maçã? e aí você pode me falar sim, eu olho a cor então a banana é o que é amarelo a maçã o que é vermelho tá, tudo bem mas e se eu te pedir para separar um maracujá? e se eu incluir uma outra fruta vermelha como por exemplo uma melancia? como que você vai criar regras para fazer essa separação? tudo o que necessita dessa grande abstração da gente não entender muito bem quais são as regras do problema a gente pode utilizar Deep Learning e devido a isso quanto mais para cá a gente está mais para dentro dessa bolinha a gente está maior a abstração que a gente vai ter do nosso problema e mais a gente vai chamar daqueles problemas caixa preta ou seja, a gente sabe que tem uma entrada e uma saída mas a gente não entende muito bem o que está acontecendo ali no meio mas talvez a gente nem precise entender só que a gente também tem que tomar cuidado porque muitas vezes isso pode trazer vieses isso pode trazer preconceitos para o nosso modelo e a gente nem perceber tem uma aula de ética em ciência de dados que eu vou deixar aqui também sugiro que vocês deem uma olhada também para tomar cuidado com os vieses dos modelos de vocês mas para não me alongar muito nisso vamos fechar essa parte explicando exatamente como funciona uma rede neural como funciona o Deep Learning e qual é o potencial desse novo assunto para ciência de dados quando eu falo de uma rede neural uma rede neural simples é basicamente isso aqui ela tem uma entrada ela tem alguns nós tem as suas conexões e vai te dar uma saída e essa estrutura de nós e conexões é uma estrutura muito similar ao neurônio que a gente já conhece com seus nós e suas conexões e o que a gente vai fazer no processo de ciência de dados no processo de aprendizado é pensar para cada uma dessas conexões qual vai ser o peso delas qual vai ser o seu viés e qual vai ser a sua função de ativação que são três palavras extremamente importantes quando você estiver entrando nesse assunto Lucas, o que que vai ser o peso? o peso basicamente vai mostrar a importância desse nó e o peso desse nó aqui a importância desse nó aqui de cima para o resultado final ou seja, o quanto essas informações aqui vão ajudar a fazer a minha previsão o viés vai ser um valor fixo para cada uma dessas conexões e um valor fixo para cada nó que a gente sempre vai adicionar para poder fazer o ajuste ali para cada um dos nós que a gente tem e a função de ativação é meio que uma adequação que a gente faz para receber os dados de forma correta nos nós de forma que a gente espera que os nós recebam os dados em geral a gente coloca ali os dados entre 0 e 1 então tem função de ativação, por exemplo a relu que eu vou falar para vocês nas próximas aulas que ele simplesmente tira os valores negativos e mantém os valores positivos tem função sigmoidal tem função tangente hiperbólica tem várias funções que a gente vai trazer isso nas próximas aulas mas é basicamente para ajustar esses dados de forma a recebê-los melhores em cada um dos nós e quando a gente fala de aprendizado o aprendizado da rede neural é exatamente o cálculo de todos esses pesos e todos esses vieses então o que a gente faz na hora de treinar um modelo? a gente pega, coloca pesos e vieses verifica quanto a gente está acertando ah, o modelo errou a gente volta e vai ajustando esses pesos testa de novo e vai fazendo isso até o nosso modelo chegar com os pesos e vieses corretos de forma a gerar o melhor resultado possível com os dados que a gente tem e para isso a gente precisa de muitos dados nesse vídeo não vou me aprofundar tanto no modelo em si mas basicamente o que a gente vai ter, por exemplo, são valores aqui e a gente vai pegar esses valores e gerar uma saída final ou por exemplo, se a gente estiver no modelo de classificação de banana a gente pode simplificar isso aqui, por exemplo ou no exemplo de classificação simples a gente pode simplificar esse problema ao máximo fazendo uma analogia como se fosse uma banana e aí você vai pedir para dois amigos seus analisarem essa banana e para um você vai perguntar olha, ela tem um cabinho? e se ele falar sim, pode ser uma banana para o outro você vai perguntar olha, ela tem um corpo amarelo? se ele falar sim, também pode ser uma banana e você vai juntar a informação desses dois vai analisar para poder pensar qual vai ser a melhor resposta na hora de classificar aquela fruta e você vai juntar essa informação dos dois para pensar qual é a melhor resposta na hora de analisar essa fruta se é uma banana ou não porque a pessoa que te fala amarelo ela pode falar amarelo para banana pode falar amarelo para maracujá pode falar amarelo para manga e para outras frutas a pessoa que fala que tem cabo pode ser uma maçã com cabo, pode ser uma pera com cabo pode ser várias outras coisas então você vai tendo cada um dos nós te dando uma certa informação e você vai pensando quais nós são mais importantes na hora de classificar cada uma das frutas e eu estou falando isso explicando de uma forma bem mais abstrata e bem mais didática para vocês entenderem basicamente o que cada neurônio vai fazer e aqui o que a gente pode pensar é que esse nó aqui vai ser muito mais importante para classificar uma banana então ele vai ter um peso maior nessa classificação então eu posso ter um peso aqui por exemplo de 0.9 esse aqui como não tem um peso tão grande de 0.1 e isso aqui que eu estou mostrando para vocês são redes extremamente simples em geral a gente tem redes com muito mais camadas o que a gente chama de camadas ocultas duas, três ou até muito mais mesmo e cada uma dessas ligações cada uma dessas conexões vai ter o seu peso e cada um desses nós vai ter os seus vieses e dentro das camadas a gente pode ter quantos nós a gente quiser posso ter três nós aqui, três nós aqui e quando a gente tem várias camadas dentro de uma rede neural é o que a gente chama de aprendizado profundo ou de Deep Learning que é o termo que a gente tem falado bastante por aí e que vocês estão vendo cada vez mais nos jornais, nas revistas e até nas entrevistas de emprego e agora só para a gente fechar quando a gente fala de rede neural a gente tem uma necessidade muito grande de dados além de ter um elevado custo computacional e muitas vezes um alto tempo de processamento por isso que as redes neurais ficaram ali por um tempo esquecidas e agora elas voltaram com tudo porque a gente tem muito dado a gente tem um custo computacional muito grande e a gente pode esperar um certo tempinho porque os computadores conseguem processar de forma paralela todas essas informações então as redes neurais elas ficaram meio esquecidas por um tempo e agora elas voltaram com tudo porque além de muitos dados a gente tem a gente fala de Big Data a gente também conseguiu um grande poder computacional e hoje a gente usa Deep Learning usa esse conhecimento para fazer coisas como reconhecimento de imagem para fazer tradução de texto e até mesmo para fazer reconhecimento de dígitos escritos à mão entre várias outras coisas e cada vez mais novas aplicações estão surgindo dentro de Deep Learning vou deixar aqui também alguns links na descrição da IBM e da Amazon falando um pouquinho sobre esse assunto e se vocês quiserem comenta aqui embaixo que a gente pode fazer um passo a passo da aplicação de Deep Learning em um caso real então é isso espero que vocês tenham gostado desse vídeo até a próxima semana e tchau E aí'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}
